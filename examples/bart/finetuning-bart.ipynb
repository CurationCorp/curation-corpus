{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning BART for abstractive text summarisation with fastai2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is also available as a [Medium post](https://medium.com/curation-corporation/fine-tuning-bart-for-abstractive-text-summarisation-with-fastai2-d7a2ad676a13). The data we are using is our recently open-sourced [Curation Corpus](https://medium.com/curation-corporation/teaching-an-ai-to-abstract-a-new-dataset-for-abstractive-auto-summarisation-5227f546caa8)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A great thing about working in NLP at the moment is being able to park a hard problem for a few weeks and discovering the community making massive amounts of progress on your behalf. I used to be overwhelmed by the challenge of just training a summarisation model to generate plausible looking text without burning through tonnes of cash on GPUs. Then [BertExtAbs](https://github.com/CurationCorp/curation-corpus/blob/master/examples/bertextabs/finetune_bertabs_walkthrough.ipynb) came along and solved that problem. Unfortunately, it still gernerated incoherent sentences sometimes and had a habit of confusing entities in an article. You certainly couldn't trust it to convey the facts of an article reliably.\n",
    "\n",
    "Enter BART (Bidirectional and Auto-Regressive Transformers). Here we have a model that generates staggeringly good summaries and has a wonderful implementation from [Sam Shleifer at HuggingFace](https://github.com/huggingface/transformers/tree/master/examples/summarization/bart). It's still a work in progress, but after digging around in the Transformers pull requests and with help from [Morgan McGuire's FastHugs notebook](https://github.com/morganmcg1/fasthugs) I have put together this notebook for fine-tuning BART and generating summaries. Feedback welcome!\n",
    "\n",
    "I should mention that this a big model requiring big inputs. For fine-tuning I've been able to get a batch size of 4 and a maximum sequence length of 512 on an AWS P3.2xlarge (~£4 an hour). There is ongoing work to reduce the memory requirements at HuggingFace and I'll update this accordingly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin with a bunch of imports and an args object for storing variables we will need. We'll be fine-tuning the model on the Curation Corpus of abstractive text summaries. We load it into a dataframe using Pandas. For more information about how to access this dataset for your own purposes please see our [article introducing the dataset](https://medium.com/curation-corporation/teaching-an-ai-to-abstract-a-new-dataset-for-abstractive-auto-summarisation-5227f546caa8)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import logging\n",
    "logging.getLogger().setLevel(100)\n",
    "from fastprogress import progress_bar\n",
    "from fastai2.basics import *\n",
    "from fastai2.data import *\n",
    "from fastai2.text.all import *\n",
    "from fastai2.callback.all import *\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import PreTrainedTokenizer, BartTokenizer, BartForConditionalGeneration, BartConfig \n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully we will be able to increase our batch size and/or maximum sequence lengths when some pull requests to reduce the model's memory footprint get merged into the Transformers repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Namespace:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "        \n",
    "args = Namespace(\n",
    "    batch_size=8,\n",
    "    max_seq_len=512,\n",
    "    data_path=\"../data/private_dataset.file\",\n",
    "    device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"), # ('cpu'),\n",
    "    stories_folder='../data/my_own_stories',\n",
    "    subset=1024,\n",
    "    test_pct=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = pd.read_feather(args.data_path).iloc[:args.subset]\n",
    "ds = ds[ds['summary'] != '']\n",
    "train_ds, test_ds = train_test_split(ds, test_size=args.test_pct, random_state=42)\n",
    "valid_ds, test_ds = train_test_split(test_ds, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To pass our data to the model in our fastai2 learner object we need a dataloader. To create a dataloader we need a Datasets object, batch size, and device type. To create a Datasets object, we have to pass a few things:\n",
    "- Our raw data which in our case is a Pandas dataframe\n",
    "- A list of transforms. Or to be more precise a list containing the list of transforms to perform on our inputs and a list of transforms to perform on our desired outputs. I've defined a transform below that encodes the text using the BART tokenizer. Mostly it will be the encodes class method that gets called by fastai2. However the decodes method can also be useful if you want to reverse the process.\n",
    "- We will also split our data into training and validation datasets here, using fastai2's RandomSplitter class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BartTokenizer.from_pretrained('bart-large-cnn', add_prefix_space=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm still exploring whether it is necessary to pass any of the masks and other ids manually or if it is handled for us. Any advice here would be much appreciated!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransform(Transform):\n",
    "    def __init__(self, tokenizer:PreTrainedTokenizer, column:string):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.column = column\n",
    "        \n",
    "    def encodes(self, inp):  \n",
    "        tokenized = self.tokenizer.batch_encode_plus(\n",
    "            [list(inp[self.column])],\n",
    "            max_length=args.max_seq_len, \n",
    "            pad_to_max_length=True, \n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return TensorText(tokenized['input_ids']).squeeze()\n",
    "        \n",
    "    def decodes(self, encoded):\n",
    "        decoded = [\n",
    "            self.tokenizer.decode(\n",
    "                o, \n",
    "                skip_special_tokens=True, \n",
    "                clean_up_tokenization_spaces=False\n",
    "            ) for o in encoded\n",
    "        ]\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tfms = [DataTransform(tokenizer, column='text')]\n",
    "y_tfms = [DataTransform(tokenizer, column='summary')]\n",
    "dss = Datasets(\n",
    "    train_ds, \n",
    "    tfms=[x_tfms, y_tfms], \n",
    "    splits=RandomSplitter(valid_pct=0.1)(range(train_ds.shape[0]))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = dss.dataloaders(bs=args.batch_size, device=args.device.type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function lets us choose between loading the model architecture with Facebook's pre-trained weights, the model architecture with our own weights stored locally, or the model architecture with no pre-training at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hf_model(config, pretrained=False, path=None): \n",
    "    if pretrained:    \n",
    "        if path:\n",
    "            model = BartForConditionalGeneration.from_pretrained(\n",
    "                \"bart-large-cnn\", \n",
    "                state_dict=torch.load(path, map_location=torch.device(args.device)), \n",
    "                config=config\n",
    "            )\n",
    "        else: \n",
    "            model = BartForConditionalGeneration.from_pretrained(\"bart-large-cnn\", config=config)\n",
    "    else:\n",
    "        model = BartForConditionalGeneration()\n",
    "\n",
    "    return model.to(args.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model will return a lot of different things, but we only want the weights to calculate the loss when training, so we will wrap the model in this class to control what gets passed to the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastaiWrapper(Module):\n",
    "    def __init__(self):\n",
    "        self.config = BartConfig(vocab_size=50264, output_past=True)\n",
    "        self.bart = load_hf_model(config=self.config, pretrained=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = self.bart(x)[0]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can think of seq2seq tasks as a series of attempts to categorise which word should come next. Cross entropy loss is a pretty good loss function for this use case. We want to normalise it by how many non padding words are in each sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummarisationLoss(Module):\n",
    "    def __init__(self):\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, output, target):\n",
    "        x = F.log_softmax(output, dim=-1)\n",
    "        norm = (target != 1).data.sum()\n",
    "        return self.criterion(x.contiguous().view(-1, x.size(-1)), target.contiguous().view(-1)) / norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When fine-tuning the model we will start by just training the top linear layer, then the decoder, and then the encoder (though I'll leave the latter as it is). fastai2 provides an easy way to split the model up into groups with frozen or unfrozen parameters.\n",
    "\n",
    "I've been experimenting with half precision training. In theory this will save a lot of memory. However, I find my loss quickly becomes a bunch of nans. This may be an issue with HuggingFace's implementation or it may be an issue with my code. I'll update if I work out how to get fp16() working. Do let me know if you have any ideas!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bart_splitter(model):\n",
    "    return [\n",
    "        params(model.bart.model.encoder), \n",
    "        params(model.bart.model.decoder),\n",
    "        params(model.bart.lm_head)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(\n",
    "    dls, \n",
    "    FastaiWrapper(), \n",
    "    loss_func=SummarisationLoss(), \n",
    "    opt_func=ranger,\n",
    "    splitter=bart_splitter\n",
    ")#.to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.freeze_to(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SuggestedLRs(lr_min=0.017378008365631102, lr_steep=1.0964781722577754e-06)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.001305</td>\n",
       "      <td>0.001612</td>\n",
       "      <td>01:47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_flat_cos(\n",
    "    1,\n",
    "    lr=1e-2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.freeze_to(-2)\n",
    "learn.dls.train.bs = args.batch_size//2\n",
    "learn.dls.valid.bs = args.batch_size//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.001866</td>\n",
       "      <td>0.001790</td>\n",
       "      <td>01:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.001834</td>\n",
       "      <td>0.001778</td>\n",
       "      <td>01:41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_flat_cos(\n",
    "    2,\n",
    "    lr=1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.export('../models/fintuned_bart.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = load_learner('../models/fintuned_bart.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code for generating the summaries comes from [Sam Shleifer's example in the Transformers repository](https://github.com/huggingface/transformers/blob/master/examples/summarization/bart/evaluate_cnn.py). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i : i + n]\n",
    "\n",
    "def generate_summaries(lns, out_file, batch_size=4):\n",
    "    dec = []\n",
    "    for batch in progress_bar(list(chunks(lns, batch_size))):\n",
    "        dct = tokenizer.batch_encode_plus(\n",
    "            batch, \n",
    "            max_length=1024, \n",
    "            return_tensors=\"pt\", \n",
    "            pad_to_max_length=True\n",
    "        )\n",
    "        \n",
    "        summaries = learn.model.bart.to(args.device).generate(\n",
    "            input_ids=dct[\"input_ids\"].to(args.device),\n",
    "            num_beams=4,\n",
    "            length_penalty=2.0,\n",
    "            max_length=142,\n",
    "            min_length=56,\n",
    "            no_repeat_ngram_size=3,\n",
    "        )\n",
    "        \n",
    "        dec.extend([tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summaries])\n",
    "        \n",
    "    return dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='1' class='' max='1', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [1/1 00:07<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lns = [\" \" + x.rstrip() for x in list(test_ds['text'])[:8]]\n",
    "bart_sums = generate_summaries(lns, f'{args.stories_folder}/output.txt', batch_size=args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " services provider Petrofac lost out on $10 billion (8 billion pounds) worth of contracts. UK's Serious Fraud Office is investigating the company's dealings in Saudi Arabia and Iraq. Petrofac shares, which have lost nearly half of their value, fell as much as 6.4% on Tuesday.\n",
      "***************\n",
      "RhythmOne, an online advertising company, released its third annual Influencer Marketing Benchmarks Report. The report provides insights into Return on Investment (ROI) and best practices based on 76 programs. Over the course of the year, 1R ran influencer marketing campaigns. with 51 brands spanning 18 advertiser categories and employing over 8,700 influencers.\n",
      "***************\n",
      "n, a.k.a. thet-a-t-e-t, is the world's biggest, most-watched, and most-loved social network, with more than 1.2. billion users. It's the most popular social network in the world, followed by Facebook, Twitter, YouTube, Instagram, Pinterest, LinkedIn, Facebook Messenger, Skype, Google+ and more. It also has the largest audience of its kind, with one in five of its users, or more, saying they use it more than any other social networking site. It has the highest number of users in the UK, and the second-largest audience in Europe.\n",
      "***************\n",
      "Bondholders had fought for years against forced repurchase of enhanced capital note (ECN) bonds. Supreme Court said the bank had been entitled to buy back the bonds at their original issue price. The move means investors, who had originally bought bonds issued by mutuals, get lower payouts and lose out on future returns.\n",
      "***************\n",
      "aanbieders Regus en Spaces gebruikmaken de business lounges. Er is gratis koffie, water, thee én wifi. Op vertoon van hun studentenkaart kunnen ze er elke weekdag tussen 9 en 17 uur terecht.\n",
      "***************\n",
      "a is a single-legged hopping robot. It's built around a central voice coil actuator, and a pair of springs. The robot can hop for up to 19 jumps before falling over, but it's not quite ready for the big time. Disney's research lab publishes a lot of fascinating work, but sometimes it's unclear what the end-goal is.\n",
      "***************\n",
      "Spotify this week tweeted that it has reached 50 million paid subscribers. That's significantly more than competitors like Apple Music, Google Play, and Tidal. Apple Music has 20 million paying users as of December, which is impressive considering the service is less than two years old.\n",
      "***************\n",
      "U.S. President-elect Donald Trump is expected to favor fewer restrictions on oil and gas activity. This will allow more room for technological innovation, analysts say. Clinton administration was expected to continue Obama administration's policy of increasing regulations of methane emissions from oil, gas, and renewable energy.\n",
      "***************\n"
     ]
    }
   ],
   "source": [
    "for s in bart_sums[:8]:\n",
    "    print(s)\n",
    "    print(\"***************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py374",
   "language": "python",
   "name": "py374"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
